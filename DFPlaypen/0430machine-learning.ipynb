{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.io as io\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "\n",
    "def verbose(text):    \n",
    "    clear_output(wait=True)    \n",
    "    print(text)\n",
    "    sys.stdout.flush() \n",
    "    \n",
    "def errorfill(x, y, yerr, color=None, label=None, alpha_fill=0.3, ax=None):\n",
    "    \"\"\"\n",
    "    Makes a nice plot with error bars\n",
    "    \"\"\"\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "    if color is None:\n",
    "        color = ax._get_lines.get_next_color()\n",
    "    if np.isscalar(yerr) or len(yerr) == len(y):\n",
    "        ymin = y - yerr\n",
    "        ymax = y + yerr\n",
    "    elif len(yerr) == 2:\n",
    "        ymin, ymax = yerr\n",
    "    ax.plot(x, y, color=color, label=label)\n",
    "    ax.fill_between(x, ymax, ymin, color=color, label=label, alpha=alpha_fill)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../shareddata/ERPs.npz'\n",
    "# filename = '../shareddata/ERPs-b50.npz'\n",
    "# filename = '../shareddata/ERPs-b100.npz'\n",
    "data = np.load(filename, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_asd_ = data['ERPs_ASD'] # [participants, catch/target, C1/C2/C3/C4]\n",
    "X_typ_ = data['ERPs_TYP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_asd = X_asd_.shape[0]\n",
    "n_typ = X_typ_.shape[0]\n",
    "n_elec, n_t, _ = X_asd_[0,0,0].shape # electrodes , timesteps, trials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# average each unique condition over trials, put them all together\n",
    "# mean is over the third index (trials), then stack them together, using third index squeezed to 1,\n",
    "# so it becomes the total number of subjects\n",
    "# the + concatenates the lists\n",
    "X_all = np.zeros((2,4,n_asd+n_typ,n_t,n_elec)) # catch/target | C1/C2/C3/C4\n",
    "for k in range(4):\n",
    "    X_all[0,k] = np.concatenate([X_asd_[i,0,k].mean(2, keepdims=True)\n",
    "                                 for i in range(n_asd)]+\n",
    "                                [X_typ_[i,0,k].mean(2, keepdims=True)\n",
    "                                 for i in range(n_typ)],2).T\n",
    "    X_all[1,k] = np.concatenate([X_asd_[i,1,k].mean(2, keepdims=True)\n",
    "                                 for i in range(n_asd)]+\n",
    "                                [X_typ_[i,1,k].mean(2, keepdims=True)\n",
    "                                 for i in range(n_typ)],2).T\n",
    "# Find the subjects that do not have any NaN    \n",
    "idx = np.isnan(X_all.mean((0,1,3,4)))==False\n",
    "X_all = X_all[:,:,idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# exercise: do my alternative X_all\n",
    "# means separately, concatenate, remove NaN subjects\n",
    "\n",
    "def temp_mean(X_):\n",
    "    \"mean over trials\"\n",
    "    X_ret = np.zeros((X_.shape+X_.flat[0].shape[0:-1])) # all dims except last\n",
    "    for ijk in np.ndindex(X_.shape):\n",
    "        X_ret[ijk] = np.mean(X_[ijk],axis=(X_.ndim-1)) #mean over last dimension\n",
    "    return X_ret\n",
    "    \n",
    "temp = np.concatenate((temp_mean(X_asd_),temp_mean(X_typ_)),axis=0)\n",
    "# find subjects that do not have nan values anywhere\n",
    "idx_subj_notnan = np.logical_not(np.any( np.isnan(temp), axis=tuple(range(1,temp.ndim))))\n",
    "# keep only good subjects\n",
    "X_all_alt = temp[idx_subj_notnan,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "def temp_mean_super(X_in,X_):\n",
    "    \"mean over trials\"\n",
    "    for ijk in np.ndindex(X_.shape):\n",
    "         #X_in[ijk]=np.mean(X_[ijk],axis=(X_.ndim-1)) #mean over last dimension \n",
    "         np.mean(X_[ijk],axis=(X_.ndim-1),out=X_in[ijk] )   \n",
    "    return\n",
    "nasd =  X_asd_.shape[0]\n",
    "nsubj = nasd + X_typ_.shape[0]\n",
    "\n",
    "shape_out = (nsubj,*(X_asd_.shape[1:]+X_asd_.flat[0].shape[0:-1]))\n",
    "mydtype = X_asd_.flat[0].dtype \n",
    "temp = np.empty( shape_out , dtype=mydtype )\n",
    "\n",
    "temp_mean_super(temp[0:nasd,],X_asd_)\n",
    "temp_mean_super(temp[nasd:,], X_typ_)\n",
    "# find subjects that do not have nan values anywhere\n",
    "idx_subj_notnan = np.logical_not(np.any( np.isnan(temp), axis=tuple(range(1,temp.ndim))))\n",
    "# keep only good subjects\n",
    "X_all_alt2 = temp[idx_subj_notnan,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def temp_mean_extreme(X_ret,X_):\n",
    "    \"mean over trials\"\n",
    "    nopes = (slice(None),slice(None))\n",
    "    for ijk in np.ndindex(X_.shape):\n",
    "        #print(nopes+ijk)\n",
    "        #Xfill = X_ret[nopes+ijk]\n",
    "        #np.mean(X_[ijk],axis=(X_.ndim-1),out=Xfill )\n",
    "        X_ret[nopes+ijk]=np.mean(X_[ijk],axis=(X_.ndim-1))\n",
    "    return\n",
    "nasd =  X_asd_.shape[0]\n",
    "nsubj = nasd + X_typ_.shape[0]\n",
    "\n",
    "shape_out = (X_asd_.flat[0].shape[0:-1]+(nsubj,)+X_asd_.shape[1:])\n",
    "mydtype = X_asd_.flat[0].dtype \n",
    "temp = np.empty( shape_out , dtype=mydtype )\n",
    "\n",
    "temp_mean_extreme(temp[:,:,0:nasd,:,:],X_asd_)\n",
    "temp_mean_extreme(temp[:,:,nasd:,:,:], X_typ_)\n",
    "# find subjects that do not have nan values anywhere\n",
    "idx_subj_notnan = np.logical_not(np.any( np.isnan(temp), axis=tuple(range(1,temp.ndim))))\n",
    "# keep only good subjects\n",
    "X_all_alt2 = temp[idx_subj_notnan,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that they are the same\n",
    "sames = np.full(X_all.shape,False)\n",
    "for ijetc in np.ndindex(X_all.shape):\n",
    "    (i_tc, i_conds, i_par, i_t, i_elec)=ijetc\n",
    "    sames[ijetc] = X_all[ijetc] == X_all_alt2[i_par,i_tc,i_conds,i_elec,i_t]\n",
    "\n",
    "print(f\"Test if they are exactly the same -> {sames.all()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate((np.zeros(n_asd),np.ones(n_typ))) # training labels\n",
    "y = y[idx]\n",
    "# target/catch , conditions, participants, timebins, electrodes\n",
    "n_tc, n_conds, n_participants, n_t, n_elec = X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of raw data for one subject and one condition\n",
    "n_t*n_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "lr = 1\n",
    "# number of components\n",
    "test_pca = np.arange(1,38,1)\n",
    "# parameter thingy\n",
    "test_C = np.logspace(-5, 0, 6)\n",
    "n_C = test_C.shape[0]\n",
    "n_pca = test_pca.shape[0]\n",
    "\n",
    "if lr:\n",
    "    clf = LogisticRegression(max_iter=5000, tol=1.0)\n",
    "    param_grid = {\n",
    "        'pca__n_components': test_pca,\n",
    "        'clf__C': test_C,\n",
    "    }\n",
    "else:\n",
    "    clf = LinearDiscriminantAnalysis(tol=1e-3)\n",
    "    param_grid = {\n",
    "        'pca__n_components': test_pca,\n",
    "    }\n",
    "\n",
    "# clf = QuadraticDiscriminantAnalysis(tol=1e-3)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('clf', clf)])\n",
    "# search = GridSearchCV(pipe, param_grid, cv=5, n_jobs=1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_all = np.empty((n_tc,n_conds), dtype=object)\n",
    "for i in range(n_tc):\n",
    "    for j in range(n_conds):\n",
    "        X = X_all[i,j].reshape(n_participants,n_t*n_elec)**2\n",
    "        \n",
    "        # using also squared signal improve performances\n",
    "        #X = np.concatenate([X_all[i,j].reshape(n_participants,n_t*n_elec),\n",
    "        #                   X_all[i,j].reshape(n_participants,n_t*n_elec)**2],\n",
    "        #                   axis=1)\n",
    "        \n",
    "        X -= X.mean(0, keepdims=True)\n",
    "        X /= X.std(0, keepdims=True)\n",
    "        search_all[i,j] = GridSearchCV(pipe, param_grid,\n",
    "                                       cv=5, n_jobs=15, verbose=0)\n",
    "        search_all[i,j].fit(X, y);\n",
    "        verbose('%i,%i'%(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(n_tc,n_conds,figsize=(12,6))\n",
    "for i in range(n_tc):\n",
    "    for j in range(n_conds):\n",
    "        print(\"Best parameter (CV score=%0.3f):\" % search_all[i,j].best_score_)\n",
    "        print(search_all[i,j].best_params_)\n",
    "        if lr:\n",
    "            for k in range(test_C.shape[0]):\n",
    "                errorfill(test_pca,search_all[i,j].\n",
    "                          cv_results_['mean_test_score']\n",
    "                          .reshape(test_C.shape[0],test_pca.shape[0])[k].T,\n",
    "                          search_all[i,j].cv_results_['std_test_score']\n",
    "                          .reshape(test_C.shape[0],test_pca.shape[0])[k].T\n",
    "                              /np.sqrt(5), ax=ax[i,j])\n",
    "                ax[i,j].set_ylim(0.4,0.9)\n",
    "            \n",
    "        else:\n",
    "            errorfill(test_pca,search_all[i,j].cv_results_['mean_test_score'],\n",
    "                      search_all[i,j].cv_results_['std_test_score']/np.sqrt(5),\n",
    "                      ax=ax[i,j])    \n",
    "        ax[i,j].set_ylim(0.4,0.95)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to try\n",
    "- Concatenate the conditions C1/C2/C3/C4\n",
    "- Use non-linear methods like kernel methods (but we are already overfitting)\n",
    "- Craft our own Quadratic Discriminant Analysis with diagonal cov matrices\n",
    "- Use L1 penalty + logistic regression (highly relevant to find a relevant low\n",
    "dimensional space)\n",
    "- Use brute force PCA components selection (using all intervals [i,j])\n",
    "- do channel selection and time windows selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling all conditions together (and time windows selection ??) \n",
    "Xct = np.zeros(2, dtype=object)\n",
    "Xct[0] = np.concatenate([X_all[0,j].reshape(n_participants,n_t*n_elec)\n",
    "                     for j in range(n_conds)], 1)\n",
    "Xct[1] = np.concatenate([X_all[1,j].reshape(n_participants,n_t*n_elec)\n",
    "                     for j in range(n_conds)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "test_pca = np.array([32])#np.arange(5,38,5)#np.array([15,38])#\n",
    "test_C = np.logspace(-3, 0, 41)\n",
    "n_C = test_C.shape[0]\n",
    "n_pca = test_pca.shape[0]\n",
    "\n",
    "clf = LogisticRegression(penalty='l1', solver='liblinear', max_iter=5000, tol=1.0)\n",
    "param_grid = {\n",
    "    'pca__n_components': test_pca,\n",
    "    'clf__C': test_C,\n",
    "}\n",
    "\n",
    "# clf = QuadraticDiscriminantAnalysis(tol=1e-3)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('clf', clf)])\n",
    "# search = GridSearchCV(pipe, param_grid, cv=5, n_jobs=1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_l1_all = np.zeros(n_tc, dtype=object)\n",
    "for i in range(n_tc):\n",
    "    #X = Xct[i]**2\n",
    "    # using also squared signal improve performances\n",
    "    X = np.concatenate([Xct[i],Xct[i]**2], axis=1)\n",
    "\n",
    "    X -= X.mean(0, keepdims=True)\n",
    "    X /= X.std(0, keepdims=True)\n",
    "    search_l1_all[i] = GridSearchCV(pipe, param_grid,\n",
    "                                   cv=5, n_jobs=15, verbose=0)\n",
    "    search_l1_all[i].fit(X, y);\n",
    "    verbose('%i'%(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, n_tc, figsize=(12,3))\n",
    "for i in range(n_tc):\n",
    "    print(\"Best parameter (CV score=%0.3f):\" % search_l1_all[i].best_score_)\n",
    "    print(search_l1_all[i].best_params_)\n",
    "    for k in range(test_pca.shape[0]):\n",
    "        errorfill(test_C,search_l1_all[i].\n",
    "                  cv_results_['mean_test_score']\n",
    "                  .reshape(test_C.shape[0],test_pca.shape[0])[:,k],\n",
    "                  search_l1_all[i].cv_results_['std_test_score']\n",
    "                  .reshape(test_C.shape[0],test_pca.shape[0])[:,k]/np.sqrt(5),\n",
    "                  ax=ax[i])\n",
    "\n",
    "    ax[i].set_xscale('log')\n",
    "    ax[i].set_ylim(0.45,0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_tc):\n",
    "    #print(np.abs(search_l1_all[i].best_estimator_['clf'].coef_))\n",
    "    print(np.sum(np.abs(search_l1_all[i].best_estimator_['clf'].coef_)>0.5e-2))\n",
    "    plt.plot(search_l1_all[i].best_estimator_['clf'].coef_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
